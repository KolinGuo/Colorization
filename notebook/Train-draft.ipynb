{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dirty-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Colorization/src\")  # Append path to src\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minute-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.dataset import load_trainval_dataset, load_predict_dataset\n",
    "from networks.models import get_model\n",
    "from networks.losses import get_loss_func\n",
    "from networks.metrics import AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "danish-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val Dataset \"train2017\" loaded: 118287 samples, 3697 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "root = '/Colorization/data/train2017'\n",
    "annFile = '/Colorization/data/annotations/instances_train2017.json'\n",
    "batch_size = 32\n",
    "train_dataset = load_trainval_dataset(root, annFile, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "violent-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 1, 256, 256]) torch.Size([32, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i, (img_l, y) in enumerate(train_dataset):\n",
    "    print(img_l.shape, y.shape)\n",
    "    \n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "crude-turning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Dataset \"test2017\" loaded: 40670 samples, 40670 batches\n"
     ]
    }
   ],
   "source": [
    "test_root = '/Colorization/data/test2017'\n",
    "batch_size = 1\n",
    "test_dataset = load_predict_dataset(test_root, None, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "casual-paint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Colorization/data/test2017/000000000001.jpg',) torch.Size([1, 1, 480, 640]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000016.jpg',) torch.Size([1, 1, 640, 480]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000019.jpg',) torch.Size([1, 1, 427, 640]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000057.jpg',) torch.Size([1, 1, 427, 640]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000063.jpg',) torch.Size([1, 1, 480, 640]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000069.jpg',) torch.Size([1, 1, 429, 640]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000080.jpg',) torch.Size([1, 1, 427, 640]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000090.jpg',) torch.Size([1, 1, 429, 640]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000106.jpg',) torch.Size([1, 1, 426, 640]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000108.jpg',) torch.Size([1, 1, 427, 640]) torch.Size([1, 1, 256, 256])\n",
      "('/Colorization/data/test2017/000000000128.jpg',) torch.Size([1, 1, 640, 490]) torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i, (img_path, img_l, y) in enumerate(test_dataset):\n",
    "    print(img_path, img_l.shape, y.shape)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "governmental-piece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000000000128'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.basename(img_path[0]).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "informative-table",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c687a21cfd4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Colorization/data/val2017'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mval_annFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Colorization/data/annotations/instances_val2017.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_annFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "val_root = '/Colorization/data/val2017'\n",
    "val_annFile = '/Colorization/data/annotations/instances_val2017.json'\n",
    "val_dataset = load_dataset(val_root, val_annFile, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welsh-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "painted-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_str = 'eccv16_pretrained'\n",
    "model_str = 'eccv16'\n",
    "model = get_model(model_str).to(pytorch_device)\n",
    "\n",
    "loss_func_str = 'MSELoss_Vibrant'\n",
    "color_vivid_gamma = 2.0\n",
    "loss_func = get_loss_func(loss_func_str, None, color_vivid_gamma)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=3*1e-5,\n",
    "                             betas=(0.9,0.99),\n",
    "                             weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "growing-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('eccv16', 3697, 157)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.name, len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "latin-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3455: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.3382e+08, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.8991, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "## Eval metric\n",
    "auc_metric = AUC(step_size = 1.0,\n",
    "                 device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "for i, (img_l, y) in enumerate(val_dataset):\n",
    "    img_l = img_l.to(pytorch_device)\n",
    "    y = y.to(pytorch_device)\n",
    "    y_pred = model(img_l)\n",
    "#     print(y_pred.shape, y.shape)\n",
    "    loss = loss_func(y_pred, y)\n",
    "    #loss = loss_func(model.normalize_ab(y_pred), model.normalize_ab(y))\n",
    "    print(i, loss)\n",
    "    \n",
    "#     print(np.max(y_pred.cpu().detach().numpy()), np.min(y_pred.cpu().detach().numpy()))\n",
    "#     diff = y_pred.cpu() - y.cpu()\n",
    "#     print(diff.detach().numpy().shape)\n",
    "#     diff = np.linalg.norm(diff.detach().numpy(), axis=1)\n",
    "#     print(diff.shape)\n",
    "#     diff_torch = torch.norm(y_pred-y, dim=1)\n",
    "#     print(diff_torch.shape)\n",
    "#     np.testing.assert_allclose(diff_torch.cpu().detach().numpy(), diff)\n",
    "#     print(np.max(diff), np.min(diff))\n",
    "#     print('cnt: ', np.count_nonzero(diff <= 150))\n",
    "#     loss = np.sum(diff) / 2 / img_l.shape[0]\n",
    "#     print(loss)\n",
    "    \n",
    "#     diff = y.cpu() - y.cpu()\n",
    "#     print(diff.detach().numpy().shape)\n",
    "#     diff = np.linalg.norm(diff.detach().numpy(), axis=1) ** 2\n",
    "#     print(diff.shape)\n",
    "#     print(np.max(diff), np.min(diff))\n",
    "#     print('cnt: ', np.count_nonzero(diff <= 0)/ np.prod(diff.shape))\n",
    "#     loss = np.sum(diff) / 2 / img_l.shape[0]\n",
    "#     print(loss)\n",
    "    \n",
    "    auc_metric.reset()\n",
    "    auc_metric.update((y_pred, y))\n",
    "    break\n",
    "#     roc_auc_metric.reset()\n",
    "#     roc_auc_metric.update((y_pred.reshape(-1), y.reshape(-1)))\n",
    "#     print(roc_auc_metric.compute())\n",
    "\n",
    "print(auc_metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accurate-haiti",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2, 256, 256]),\n",
       " torch.Size([32, 256, 256]),\n",
       " torch.Size([32, 256, 256]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape, y_pred[:,0,:,:].shape, y_pred[:,1,:,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-cooperation",
   "metadata": {},
   "source": [
    "### Outdated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "legendary-space",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2, 256, 256]) torch.Size([32, 2, 256, 256])\n",
      "torch.Size([32, 256, 256])\n",
      "torch.Size([32, 256, 256])\n",
      "Test\n",
      "torch.Size([32, 256, 256]) torch.Size([32, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(9.6977e+08, device='cuda:0', grad_fn=<MseLossBackward>),\n",
       " tensor(9.6977e+08, device='cuda:0', grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MSELoss_Vibrant testing\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(y_pred.shape, y.shape)\n",
    "c_pred_torch = torch.norm(y_pred, dim=1)\n",
    "print(c_pred_torch.shape)\n",
    "c_pred_einsum = torch.sqrt(torch.einsum('abcde,acfde->abfde', y_pred[:,None,...], y_pred[:,:,None,...]))\n",
    "print(c_pred_einsum[:,0,0,:,:].shape)\n",
    "np.testing.assert_allclose(c_pred_einsum[:,0,0,:,:].cpu().detach().numpy(), c_pred_torch.cpu().detach().numpy())\n",
    "\n",
    "print('Test')\n",
    "c_pred = torch.norm(y_pred, dim=1)\n",
    "c_true = torch.norm(y, dim=1)\n",
    "print(c_pred.shape, c_true.shape)\n",
    "F.mse_loss(c_pred, c_true, reduction='sum'), torch.sum((c_pred - c_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "terminal-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([151])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'diff_torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f5aa3c9ae036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'diff_torch' is not defined"
     ]
    }
   ],
   "source": [
    "step = 1.0\n",
    "thresholds = torch.arange(0, 150+step, step=step).to(pytorch_device)\n",
    "print(type(thresholds))\n",
    "print(thresholds.shape)\n",
    "print(thresholds[None,None,None,...].shape, diff_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-camping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sensitive-vinyl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([151])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(thresholds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "regulated-breed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.numel(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC_metric(y_pred: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    step = 1\n",
    "    thresholds = torch.arange(0, 150+step, step=step).to(pytorch_device)\n",
    "    dist = torch.norm(y_pred-y, dim=1)\n",
    "    pos_cnt = torch.sum(torch.le(dist[...,None], thresholds[None,None,None,...]), dim=(0,1,2))\n",
    "    tot_cnt += torch.numel(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "auburn-possible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([     0,    466,   1834,   3982,   7190,  11262,  16062,  21437,  27440,\n",
      "         33744,  40254,  47248,  54968,  62934,  71296,  79735,  88089,  96167,\n",
      "        104364, 112671, 120886, 129022, 137113, 144687, 151826, 158527, 164690,\n",
      "        170659, 176628, 182180, 187559, 192688, 197459, 201887, 206149, 210065,\n",
      "        213915, 217441, 220753, 223982, 227035, 229954, 232607, 235138, 237417,\n",
      "        239509, 241403, 243226, 244882, 246366, 247733, 248956, 249974, 250968,\n",
      "        251866, 252824, 253664, 254472, 255214, 255913, 256558, 257130, 257654,\n",
      "        258134, 258536, 258935, 259317, 259607, 259860, 260079, 260280, 260475,\n",
      "        260645, 260810, 260948, 261070, 261195, 261323, 261437, 261531, 261607,\n",
      "        261682, 261744, 261802, 261864, 261905, 261953, 261981, 262009, 262039,\n",
      "        262064, 262084, 262099, 262115, 262122, 262129, 262136, 262140, 262141,\n",
      "        262143, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144],\n",
      "       device='cuda:0') 262144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8386, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = diff_torch\n",
    "pos_cnt = torch.sum(torch.le(dist[...,None], thresholds[None,None,None,...]), dim=(0,1,2))\n",
    "tot_cnt = torch.numel(dist)\n",
    "print(pos_cnt, tot_cnt)\n",
    "torch.sum(pos_cnt / tot_cnt) / torch.numel(pos_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "designing-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.dataset import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reasonable-turtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-lz4yu38j because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "root = '/Colorization/data/train2017'\n",
    "annFile = '/Colorization/data/annotations/instances_train2017.json'\n",
    "batch_size = 64\n",
    "train_dataset = load_dataset(root, annFile, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hazardous-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pytorch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "otherwise-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.eccv16 import eccv16\n",
    "\n",
    "model = eccv16(pretrained=True).to(pytorch_device)\n",
    "    \n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3*1e-5,betas=(0.9,0.99),weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hungry-negative",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3455: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48900.44\n",
      "0 2391416832.0 Time: 2.087425708770752 sec\n",
      "45685.168\n",
      "1 2087234432.0 Time: 8.31327772140503 sec\n",
      "50968.28\n",
      "2 2597967104.0 Time: 6.674348592758179 sec\n",
      "42881.805\n",
      "3 1838942208.0 Time: 6.67439341545105 sec\n",
      "45425.117\n",
      "4 2063548032.0 Time: 6.694308042526245 sec\n",
      "40564.984\n",
      "5 1645593216.0 Time: 6.673346757888794 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-47112883c3f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimg_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/torchvision/datasets/coco.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~Colorization/src/networks/dataset.py\u001b[0m in \u001b[0;36minput_transforms\u001b[0;34m(img_rgb_orig, target, HW, resample)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimg_rgb_rs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rgb_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mimg_lab_rs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rgb_rs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mimg_l_rs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_lab_rs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2lab\u001b[0;34m(rgb, illuminant, observer)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mStandard_illuminant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m     \"\"\"\n\u001b[0;32m-> 1092\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxyz2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb2xyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milluminant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2xyz\u001b[0;34m(rgb)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_colorarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.04045\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.055\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1.055\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m12.92\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mxyz_from_rgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for i, (img_l, y) in enumerate(train_dataset):\n",
    "    img_l = img_l.to(pytorch_device)\n",
    "    y = y.to(pytorch_device)\n",
    "    y_pred = model(img_l)\n",
    "    print(np.linalg.norm(y_pred.detach().cpu().numpy() - y.cpu().numpy()))\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    #loss = criterion(model.normalize_ab(y_pred), model.normalize_ab(y))\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(i, loss.item(), f'Time: {toc-tic} sec')\n",
    "    tic = time.time()\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-butterfly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
