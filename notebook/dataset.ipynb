{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spanish-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Colorization/src\")  # Append path to src\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coastal-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def input_trans(img_rgb_orig, HW=(256,256), resample=3):\n",
    "    # return original size L and resized L as torch Tensors\n",
    "    img_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n",
    "    \n",
    "    img_lab_rs = color.rgb2lab(img_rgb_rs)\n",
    "\n",
    "    img_l_rs = img_lab_rs[:,:,0]\n",
    "    img_ab_rs = img_lab_rs[:,:,1:]\n",
    "\n",
    "    tens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n",
    "    tens_rs_ab = torch.Tensor(img_ab_rs)[None,None,:,:]\n",
    "\n",
    "    return (tens_rs_l, tens_rs_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "worldwide-cisco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.49s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def input_transforms(img_rgb_orig, target, HW=(256,256), resample=3):\n",
    "    # return original size L and resized L as torch Tensors\n",
    "    img_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n",
    "    \n",
    "    img_lab_rs = color.rgb2lab(img_rgb_rs)\n",
    "\n",
    "    img_l_rs = img_lab_rs[:,:,0]\n",
    "    img_ab_rs = np.moveaxis(img_lab_rs[:,:,1:], -1, 0)  # (2, 256, 256)\n",
    "\n",
    "    tens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n",
    "    tens_rs_ab = torch.Tensor(img_ab_rs)[None,None,:,:]\n",
    "\n",
    "    return (tens_rs_l, tens_rs_ab)\n",
    "\n",
    "trainset = dset.CocoDetection(root='/Colorization/data/train2017', \n",
    "                              annFile='/Colorization/data/annotations/instances_train2017.json', \n",
    "                              transforms=input_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "contemporary-department",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 256, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executed-corporation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-ke0y8yrz because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "from eccv16 import eccv16\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def input_transforms(img_rgb_orig, target, HW=(256,256), resample=3):\n",
    "    # return original size L and resized L as torch Tensors\n",
    "    img_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n",
    "    \n",
    "    img_lab_rs = color.rgb2lab(img_rgb_rs)\n",
    "\n",
    "    img_l_rs = img_lab_rs[:,:,0]\n",
    "    img_ab_rs = np.moveaxis(img_lab_rs[:,:,1:], -1, 0)  # (2, 256, 256)\n",
    "\n",
    "    tens_rs_l = torch.Tensor(img_l_rs)[None,:,:]\n",
    "    tens_rs_ab = torch.Tensor(img_ab_rs)[:,:]\n",
    "\n",
    "    return (tens_rs_l, tens_rs_ab)\n",
    "\n",
    "trainset = dset.CocoDetection(root='/Colorization/data/train2017', \n",
    "                              annFile='/Colorization/data/annotations/instances_train2017.json', \n",
    "                              transforms=input_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interracial-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_device = torch.device(\"cuda:0\")#\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sized-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64)\n",
    "\n",
    "model = eccv16(pretrained=False).to(pytorch_device)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3*1e-5,betas=(0.9,0.99),weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "another-hudson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48758.016\n",
      "0 2377514240.0 Time: 2.050006151199341 sec\n",
      "45549.938\n",
      "1 2074922240.0 Time: 6.8503007888793945 sec\n",
      "50817.266\n",
      "2 2582576384.0 Time: 6.615176677703857 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4ff04aeb172f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimg_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "for i, (img_l, y) in enumerate(trainloader):\n",
    "    img_l = img_l.to(pytorch_device)\n",
    "    y = y.to(pytorch_device)\n",
    "    y_pred = model(img_l)\n",
    "    print(np.linalg.norm(y_pred.detach().cpu().numpy() - y.cpu().numpy()))\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    #loss = criterion(model.normalize_ab(y_pred), model.normalize_ab(y))\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(i, loss.item(), f'Time: {toc-tic} sec')\n",
    "    tic = time.time()\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-prescription",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "activated-liechtenstein",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 256]), torch.Size([2, 256, 256]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l, ab = trainset[0]\n",
    "l.shape, ab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eastern-agriculture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_pred = model.to('cpu')(l[None,...])\n",
    "ab_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "married-portugal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.7890e-01,  2.9074e-01,  2.9589e-01,  ..., -5.4961e-02,\n",
       "           -5.7423e-02, -5.7302e-02],\n",
       "          [ 2.9895e-01,  3.0024e-01,  3.0319e-01,  ..., -4.4786e-02,\n",
       "           -5.7571e-02, -5.7384e-02],\n",
       "          [ 2.9119e-01,  2.8268e-01,  2.8741e-01,  ..., -5.5058e-02,\n",
       "           -6.5139e-02, -6.5045e-02],\n",
       "          ...,\n",
       "          [ 1.8206e-02,  2.3610e-02,  7.9364e-03,  ...,  1.4986e-02,\n",
       "           -1.4619e-02,  4.0481e-02],\n",
       "          [ 1.3301e-02,  2.0583e-02,  1.3905e-02,  ...,  2.1085e-02,\n",
       "            4.0446e-02,  5.0372e-02],\n",
       "          [ 1.3301e-02,  1.9319e-02,  2.5926e-02,  ...,  6.3628e-02,\n",
       "            5.7654e-02,  3.9001e-02]],\n",
       " \n",
       "         [[-4.6523e-01, -4.7776e-01, -4.8582e-01,  ..., -1.5565e-01,\n",
       "           -1.5721e-01, -1.5762e-01],\n",
       "          [-4.7971e-01, -4.8441e-01, -4.9320e-01,  ..., -1.5734e-01,\n",
       "           -1.5667e-01, -1.5735e-01],\n",
       "          [-4.7720e-01, -4.7501e-01, -4.8361e-01,  ..., -1.5526e-01,\n",
       "           -1.5364e-01, -1.5402e-01],\n",
       "          ...,\n",
       "          [-1.3013e-02, -1.1206e-02,  2.2069e-02,  ..., -2.1861e-01,\n",
       "           -1.6398e-01, -1.7585e-01],\n",
       "          [-6.9699e-03, -1.2174e-02,  2.0955e-04,  ..., -1.7398e-01,\n",
       "           -1.6849e-01, -1.5176e-01],\n",
       "          [-6.9699e-03, -8.7342e-03, -2.4816e-02,  ..., -1.9015e-01,\n",
       "           -1.5724e-01, -9.1530e-02]]]),\n",
       " tensor([[[ 0.1109,  0.1109,  0.1014,  ..., -0.0109, -0.0112, -0.0112],\n",
       "          [ 0.1109,  0.1109,  0.1014,  ..., -0.0109, -0.0112, -0.0112],\n",
       "          [ 0.1004,  0.1004,  0.0926,  ..., -0.0109, -0.0113, -0.0113],\n",
       "          ...,\n",
       "          [ 0.0185,  0.0185,  0.0186,  ...,  0.1038,  0.0979,  0.0979],\n",
       "          [ 0.0199,  0.0199,  0.0197,  ...,  0.0868,  0.0796,  0.0796],\n",
       "          [ 0.0199,  0.0199,  0.0197,  ...,  0.0868,  0.0796,  0.0796]],\n",
       " \n",
       "         [[-0.0212, -0.0212, -0.0422,  ..., -0.0744, -0.0748, -0.0748],\n",
       "          [-0.0212, -0.0212, -0.0422,  ..., -0.0744, -0.0748, -0.0748],\n",
       "          [-0.0318, -0.0318, -0.0503,  ..., -0.0732, -0.0741, -0.0741],\n",
       "          ...,\n",
       "          [-0.0346, -0.0346, -0.0394,  ..., -0.0913, -0.0889, -0.0889],\n",
       "          [-0.0266, -0.0266, -0.0315,  ..., -0.0815, -0.0772, -0.0772],\n",
       "          [-0.0266, -0.0266, -0.0315,  ..., -0.0815, -0.0772, -0.0772]]],\n",
       "        grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.normalize_ab(ab), model.normalize_ab(ab_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "radical-border",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 12.1945,  12.1945,  11.1531,  ...,  -1.1940,  -1.2353,  -1.2353],\n",
       "          [ 12.1945,  12.1945,  11.1531,  ...,  -1.1940,  -1.2353,  -1.2353],\n",
       "          [ 11.0493,  11.0493,  10.1825,  ...,  -1.1962,  -1.2432,  -1.2432],\n",
       "          ...,\n",
       "          [  2.0328,   2.0328,   2.0499,  ...,  11.4133,  10.7650,  10.7650],\n",
       "          [  2.1856,   2.1856,   2.1696,  ...,   9.5490,   8.7544,   8.7544],\n",
       "          [  2.1856,   2.1856,   2.1696,  ...,   9.5490,   8.7544,   8.7544]],\n",
       " \n",
       "         [[ -2.3308,  -2.3308,  -4.6468,  ...,  -8.1821,  -8.2327,  -8.2327],\n",
       "          [ -2.3308,  -2.3308,  -4.6468,  ...,  -8.1821,  -8.2327,  -8.2327],\n",
       "          [ -3.4979,  -3.4979,  -5.5311,  ...,  -8.0524,  -8.1456,  -8.1456],\n",
       "          ...,\n",
       "          [ -3.8110,  -3.8110,  -4.3346,  ..., -10.0400,  -9.7794,  -9.7794],\n",
       "          [ -2.9274,  -2.9274,  -3.4597,  ...,  -8.9700,  -8.4904,  -8.4904],\n",
       "          [ -2.9274,  -2.9274,  -3.4597,  ...,  -8.9700,  -8.4904,  -8.4904]]],\n",
       "        grad_fn=<SelectBackward>),\n",
       " tensor([[[ 3.0679e+01,  3.1981e+01,  3.2548e+01,  ..., -6.0457e+00,\n",
       "           -6.3165e+00, -6.3032e+00],\n",
       "          [ 3.2884e+01,  3.3026e+01,  3.3351e+01,  ..., -4.9265e+00,\n",
       "           -6.3328e+00, -6.3122e+00],\n",
       "          [ 3.2031e+01,  3.1095e+01,  3.1615e+01,  ..., -6.0564e+00,\n",
       "           -7.1653e+00, -7.1550e+00],\n",
       "          ...,\n",
       "          [ 2.0027e+00,  2.5971e+00,  8.7300e-01,  ...,  1.6484e+00,\n",
       "           -1.6081e+00,  4.4529e+00],\n",
       "          [ 1.4631e+00,  2.2641e+00,  1.5296e+00,  ...,  2.3193e+00,\n",
       "            4.4490e+00,  5.5409e+00],\n",
       "          [ 1.4631e+00,  2.1251e+00,  2.8518e+00,  ...,  6.9991e+00,\n",
       "            6.3420e+00,  4.2901e+00]],\n",
       " \n",
       "         [[-5.1175e+01, -5.2554e+01, -5.3441e+01,  ..., -1.7122e+01,\n",
       "           -1.7293e+01, -1.7338e+01],\n",
       "          [-5.2768e+01, -5.3286e+01, -5.4252e+01,  ..., -1.7307e+01,\n",
       "           -1.7234e+01, -1.7308e+01],\n",
       "          [-5.2492e+01, -5.2251e+01, -5.3197e+01,  ..., -1.7078e+01,\n",
       "           -1.6900e+01, -1.6942e+01],\n",
       "          ...,\n",
       "          [-1.4314e+00, -1.2327e+00,  2.4276e+00,  ..., -2.4047e+01,\n",
       "           -1.8038e+01, -1.9344e+01],\n",
       "          [-7.6668e-01, -1.3392e+00,  2.3050e-02,  ..., -1.9138e+01,\n",
       "           -1.8534e+01, -1.6693e+01],\n",
       "          [-7.6668e-01, -9.6076e-01, -2.7297e+00,  ..., -2.0916e+01,\n",
       "           -1.7296e+01, -1.0068e+01]]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_pred[0], ab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
